{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# © Copyright IBM Corporation 2022.\n",
    "#\n",
    "# LICENSE: Apache License 2.0 (Apache-2.0)\n",
    "# http://www.apache.org/licenses/LICENSE-2.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import uuid\n",
    "import io\n",
    "import html\n",
    "import re\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "\n",
    "from filelock import FileLock\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict, Counter\n",
    "from enum import Enum\n",
    "from typing import Dict, List\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer, PretrainedConfig, PreTrainedTokenizerBase,\n",
    "                          InputFeatures, Trainer, TrainingArguments, RobertaConfig, BartConfig, DebertaConfig, pipeline)\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "\n",
    "def get_root_dir():\n",
    "    return os.path.abspath(os.path.join(__file__, os.pardir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Structure and Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Structure & Class Names\n",
    "@dataclass\n",
    "class Predictions:\n",
    "    predicted_labels: List[str]\n",
    "    ranked_classes: List[List[str]]\n",
    "    class_name_to_score: List[Dict[str, float]]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SelfTrainingSet:\n",
    "    texts: List[str] = field(default_factory=list)\n",
    "    class_names: List[str] = field(default_factory=list)\n",
    "    entailment_labels: List[str] = field(default_factory=list)\n",
    "DATASET_TO_CLASS_NAME_MAPPING = \\\n",
    "    {'20_newsgroup':\n",
    "         {'0': 'atheism', '1': 'computer graphics', '2': 'microsoft windows', '3': 'pc hardware',\n",
    "          '4': 'mac hardware', '5': 'windows x', '6': 'for sale', '7': 'cars', '8': 'motorcycles', '9': 'baseball',\n",
    "          '10': 'hockey', '11': 'cryptography', '12': 'electronics', '13': 'medicine', '14': 'space',\n",
    "          '15': 'christianity', '16': 'guns', '17': 'middle east', '18': 'politics', '19': 'religion'},\n",
    "\n",
    "     'ag_news':\n",
    "         {'Business': 'business', 'Sci/Tech': 'science and technology', 'Sports': 'sports', 'World': 'world'},\n",
    "\n",
    "     'dbpedia':\n",
    "         {'Album': 'album', 'Animal': 'animal', 'Artist': 'artist', 'Athlete': 'athlete', 'Building': 'building',\n",
    "          'Company': 'company', 'EducationalInstitution': 'educational institution', 'Film': 'film',\n",
    "          'MeanOfTransportation': 'mean of transportation', 'NaturalPlace': 'natural place',\n",
    "          'OfficeHolder': 'office holder', 'Plant': 'plant', 'Village': 'village', 'WrittenWork': 'written work'},\n",
    "\n",
    "     'imdb':\n",
    "         {'pos': 'good', 'neg': 'bad'}\n",
    "     }\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Candidate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidate_indices_per_class(all_class_names, predictions: Predictions) -> Dict[str, List[int]]:\n",
    "    diff_scores_to_second_best = \\\n",
    "        [class_name_to_score[ranked_classes[0]] - class_name_to_score[ranked_classes[1]]\n",
    "         for class_name_to_score, ranked_classes in zip(predictions.class_name_to_score, predictions.ranked_classes)]\n",
    "\n",
    "    class_name_to_sorted_candidate_idxs = {}\n",
    "    for class_name in all_class_names:\n",
    "        sorted_candidate_idxs = \\\n",
    "            [idx for idx, (predicted_class, diff_to_second_best)\n",
    "             in sorted(enumerate(zip(predictions.predicted_labels, diff_scores_to_second_best)),\n",
    "                       key=lambda x: x[1][1], reverse=True)\n",
    "             if predicted_class == class_name]\n",
    "        class_name_to_sorted_candidate_idxs[class_name] = sorted_candidate_idxs\n",
    "\n",
    "    return class_name_to_sorted_candidate_idxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingStrategy(Enum):\n",
    "    TAKE_ALL = 0\n",
    "    TAKE_RANDOM = 1\n",
    "    TAKE_SECOND = 2\n",
    "    TAKE_LAST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_examples(predictions: Predictions, class_name_to_chosen_pos_idxs: Dict[str, List[int]],\n",
    "                          negative_sampling_strategy: NegativeSamplingStrategy) -> Dict[str, List[int]]:\n",
    "\n",
    "    all_positive_idxs = [idx for class_idxs in class_name_to_chosen_pos_idxs.values() for idx in class_idxs]\n",
    "\n",
    "    class_name_to_chosen_negative_idxs = defaultdict(list)\n",
    "    for idx in all_positive_idxs:\n",
    "        example_ranked_classes = predictions.ranked_classes[idx]\n",
    "\n",
    "        if negative_sampling_strategy == NegativeSamplingStrategy.TAKE_SECOND:\n",
    "            class_name_to_chosen_negative_idxs[example_ranked_classes[1]].append(idx)\n",
    "        elif negative_sampling_strategy == NegativeSamplingStrategy.TAKE_ALL:\n",
    "            for class_name in example_ranked_classes[1:]:\n",
    "                class_name_to_chosen_negative_idxs[class_name].append(idx)\n",
    "        elif negative_sampling_strategy == NegativeSamplingStrategy.TAKE_LAST:\n",
    "            class_name_to_chosen_negative_idxs[example_ranked_classes[-1]].append(idx)\n",
    "        elif negative_sampling_strategy == NegativeSamplingStrategy.TAKE_RANDOM:\n",
    "            random_negative_class = random.choice(example_ranked_classes[1:])\n",
    "            class_name_to_chosen_negative_idxs[random_negative_class].append(idx)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown negative sampling strategy {negative_sampling_strategy}\")\n",
    "\n",
    "    return class_name_to_chosen_negative_idxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_to_csv(df, path, save_index_col=False):\n",
    "    buffer = io.StringIO()\n",
    "    df.to_csv(buffer, index=save_index_col)\n",
    "    buffer.seek(0)\n",
    "    output = buffer.getvalue()\n",
    "    buffer.close()\n",
    "    with open(path, \"w\") as text_file:\n",
    "        text_file.write(output)\n",
    "        text_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(res_file_name, agg_file_name, result_dict, setting_name_col='setting_name'):\n",
    "    def mean_str(col: pd.Series):\n",
    "        if pd.api.types.is_numeric_dtype(col):\n",
    "            return col.mean()\n",
    "        else:\n",
    "            return col.dropna().unique()[0] if col.nunique() == 1 else np.NaN\n",
    "\n",
    "    new_res_df = pd.DataFrame([result_dict])\n",
    "\n",
    "    lock_path = os.path.abspath(os.path.join(res_file_name, os.pardir, 'result_csv_files.lock'))\n",
    "    with FileLock(lock_path):\n",
    "        if os.path.isfile(res_file_name):\n",
    "            orig_df = pd.read_csv(res_file_name)\n",
    "            df = pd.concat([orig_df, new_res_df])\n",
    "            df_agg = df.groupby(by=setting_name_col).agg(mean_str).sort_values(by=['dataset_name', setting_name_col])\n",
    "            safe_to_csv(df_agg, agg_file_name, save_index_col=True)\n",
    "        else:\n",
    "            df = new_res_df\n",
    "        safe_to_csv(df, res_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_performance(predicted_labels, gold_labels, out_dir, info_dict):\n",
    "\n",
    "    accuracy = np.mean([gold_label == prediction for gold_label, prediction in zip(gold_labels, predicted_labels)])\n",
    "    evaluation_dict = {'accuracy': accuracy, 'evaluation_size': len(gold_labels)}\n",
    "\n",
    "    report = classification_report(gold_labels, predicted_labels, output_dict=True)\n",
    "    report.pop('accuracy')\n",
    "    for category, metrics in report.items():\n",
    "        if not category.endswith(\"avg\"):\n",
    "            category = f\"'{category}'\"\n",
    "\n",
    "        evaluation_dict[f\"{category} precision\"] = metrics['precision']\n",
    "        evaluation_dict[f\"{category} recall\"] = metrics['recall']\n",
    "        evaluation_dict[f\"{category} f1\"] = metrics['f1-score']\n",
    "\n",
    "    evaluation_dict = {**info_dict, **evaluation_dict}\n",
    "    logging.info(evaluation_dict)\n",
    "\n",
    "    all_copies_file = os.path.join(out_dir, \"all_copies.csv\")\n",
    "    agg_file = os.path.join(out_dir, \"aggregated.csv\")\n",
    "    save_results(all_copies_file, agg_file, evaluation_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entailment models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zero_shot_predictions(model_name, texts_to_infer, label_names, batch_size, max_length):\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "    # We initialize the tokenizer here in order to set the maximum sequence length\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, model_max_length=max_length)\n",
    "\n",
    "    classifier = pipeline(\"zero-shot-classification\", model=model_name, tokenizer=tokenizer, device=device)\n",
    "\n",
    "    ds = Dataset.from_dict({'text': texts_to_infer})\n",
    "\n",
    "    preds_list = []\n",
    "    for text, output in tqdm(zip(texts_to_infer, classifier(KeyDataset(ds, 'text'),\n",
    "                                                            batch_size=batch_size,\n",
    "                                                            candidate_labels=label_names, multi_label=True)),\n",
    "                             total=len(ds), desc=\"zero-shot inference\"):\n",
    "        preds_list.append(output)\n",
    "\n",
    "    predictions = Predictions(predicted_labels=[x['labels'][0] for x in preds_list],\n",
    "                              ranked_classes=[x['labels'] for x in preds_list],\n",
    "                              class_name_to_score=[dict(zip(x['labels'], x['scores'])) for x in preds_list])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_entailment_model(model_name, self_training_set: SelfTrainingSet, seed, learning_rate=2e-5, batch_size=32,\n",
    "                              max_length=512, num_epochs=1, hypothesis_template=\"This example is {}.\"):\n",
    "    model_id = f\"{str(uuid.uuid4())}_fine_tuned_{model_name.replace(os.sep, '_')}\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, model_max_length=max_length)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "    out_dir = os.path.join(get_root_dir(), \"output\", \"models\", str(model_id))\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    inputs = preprocess_and_tokenize(model.config, tokenizer, self_training_set, seed=seed,\n",
    "                                     hypothesis_template=hypothesis_template)\n",
    "\n",
    "    training_args = TrainingArguments(output_dir=out_dir,\n",
    "                                      overwrite_output_dir=True,\n",
    "                                      num_train_epochs=num_epochs,\n",
    "                                      per_device_train_batch_size=batch_size,\n",
    "                                      learning_rate=learning_rate)\n",
    "\n",
    "    trainer = Trainer(model=model, args=training_args, train_dataset=inputs)\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(out_dir)\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "    return out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_tokenize(model_config: PretrainedConfig, tokenizer: PreTrainedTokenizerBase,\n",
    "                            self_training_set: SelfTrainingSet, seed: int, hypothesis_template: str):\n",
    "\n",
    "    if type(model_config) not in [RobertaConfig, DebertaConfig, BartConfig]:\n",
    "        raise NotImplementedError(f\"{model_config.architectures} model is not supported\")\n",
    "\n",
    "    def get_numeric_label(label: str, model_config: PretrainedConfig):\n",
    "        \"\"\"\n",
    "        Different entailment classification models on Huggingface use different names and IDs for the textual entailment\n",
    "        labels of entailment/neutral/contradiction. Here we convert names to the appropriate model label IDs.\n",
    "        \"\"\"\n",
    "        if label in model_config.label2id:\n",
    "            return model_config.label2id[label]\n",
    "        elif label.lower() in model_config.label2id:\n",
    "            return model_config.label2id[label.lower()]\n",
    "        else:\n",
    "            raise Exception(f'The label \"{label}\" is not recognized by the model, '\n",
    "                            f'model labels are: {model_config.label2id.keys()}')\n",
    "\n",
    "    numeric_entailment_labels = [get_numeric_label(label, model_config)\n",
    "                                 for label in self_training_set.entailment_labels]\n",
    "\n",
    "    tokenized = []\n",
    "    for text, class_name, label \\\n",
    "            in zip(self_training_set.texts, self_training_set.class_names, numeric_entailment_labels):\n",
    "\n",
    "        hypothesis = hypothesis_template.format(class_name)\n",
    "        inputs = (tokenizer.encode_plus([text, hypothesis], add_special_tokens=True, padding='max_length',\n",
    "                                        truncation='only_first'))\n",
    "\n",
    "        if type(model_config) == DebertaConfig:\n",
    "            tokenized.append(InputFeatures(input_ids=inputs['input_ids'],\n",
    "                                           attention_mask=inputs['attention_mask'],\n",
    "                                           token_type_ids=inputs['token_type_ids'],\n",
    "                                           label=label))\n",
    "        elif type(model_config) in [RobertaConfig, BartConfig]:\n",
    "            tokenized.append(InputFeatures(input_ids=inputs['input_ids'],\n",
    "                                           attention_mask=inputs['attention_mask'],\n",
    "                                           label=label))\n",
    "\n",
    "    random.Random(seed).shuffle(tokenized)\n",
    "    return tokenized"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = './datasets'\n",
    "RAW_DIR = os.path.join(OUT_DIR, 'raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_name(dataset_name, csv_label_name):\n",
    "    if dataset_name in DATASET_TO_CLASS_NAME_MAPPING:\n",
    "        return DATASET_TO_CLASS_NAME_MAPPING[dataset_name][str(csv_label_name)]\n",
    "    else:\n",
    "        return csv_label_name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_20_newsgroup():\n",
    "    def clean_text(x):\n",
    "        x = re.sub('#\\S+;', '&\\g<0>', x)\n",
    "        x = re.sub('(\\w+)\\\\\\(\\w+)', '\\g<1> \\g<2>', x)\n",
    "        x = x.replace('quot;', '&quot;')\n",
    "        x = x.replace('amp;', '&amp;')\n",
    "        x = x.replace('\\$', '$')\n",
    "        x = x.replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \")\n",
    "        x = x.strip()\n",
    "        while x.endswith(\"\\\\\"):\n",
    "            x = x[:-1]\n",
    "        return html.unescape(x)\n",
    "\n",
    "    dataset_name = \"20_newsgroup\"\n",
    "    dataset_out_dir = os.path.join(OUT_DIR, dataset_name)\n",
    "    os.makedirs(dataset_out_dir, exist_ok=True)\n",
    "\n",
    "    newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "    train_df = pd.DataFrame({\"text\": newsgroups_train[\"data\"], \"label\": newsgroups_train[\"target\"]})\n",
    "    train_df[\"text\"] = train_df[\"text\"].apply(lambda x: clean_text(x))\n",
    "    train_df[\"label\"] = train_df[\"label\"].apply(lambda x: get_label_name(dataset_name, x))\n",
    "    train_df.to_csv(os.path.join(dataset_out_dir, \"unlabeled.csv\"), index=False)\n",
    "    logging.info(f\"20_newsgroup unlabeled file created with {len(train_df)} samples\")\n",
    "    newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "    test_df = pd.DataFrame({\"text\": newsgroups_test[\"data\"], \"label\": newsgroups_test[\"target\"]})\n",
    "    test_df[\"text\"] = test_df[\"text\"].apply(lambda x: clean_text(x))\n",
    "    test_df[\"label\"] = test_df[\"label\"].apply(lambda x: get_label_name(dataset_name, x))\n",
    "    test_df.to_csv(os.path.join(dataset_out_dir, \"test.csv\"), index=False)\n",
    "\n",
    "    with open(os.path.join(dataset_out_dir, 'class_names.txt'), 'w') as f:\n",
    "        f.writelines([class_name+'\\n' for class_name in sorted(test_df[\"label\"].unique())])\n",
    "\n",
    "    logging.info(f\"20_newsgroup test file created with {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ag_news_dbpedia_yahoo():\n",
    "    def clean_text(x):\n",
    "        x = re.sub('#\\S+;', '&\\g<0>', x)\n",
    "        x = re.sub('(\\w+)\\\\\\(\\w+)', '\\g<1> \\g<2>', x)\n",
    "        x = x.replace('quot;', '&quot;')\n",
    "        x = x.replace('amp;', '&amp;')\n",
    "        x = x.replace('\\$', '$')\n",
    "        x = ' '.join(x.split())\n",
    "        while x.endswith(\"\\\\\"):\n",
    "            x = x[:-1]\n",
    "        return html.unescape(x)\n",
    "\n",
    "    dataset_to_columns = {'ag_news': [\"label\", \"title\", \"text\"],\n",
    "                          'dbpedia': [\"label\", \"title\", \"text\"],\n",
    "                          'yahoo_answers': ['label', 'question_title', 'question_content', 'answer']}\n",
    "\n",
    "    for dataset, column_names in dataset_to_columns.items():\n",
    "        logging.info(f'processing {dataset} csv files')\n",
    "        raw_path = os.path.join(RAW_DIR, dataset, f'{dataset}_csv')\n",
    "        with open(os.path.join(raw_path, 'classes.txt'), 'r') as f:\n",
    "            idx_to_class_name = dict(enumerate([get_label_name(dataset, row.strip())\n",
    "                                                for row in f.readlines()]))\n",
    "\n",
    "        dataset_out_dir = os.path.join(OUT_DIR, dataset)\n",
    "        os.makedirs(dataset_out_dir, exist_ok=True)\n",
    "\n",
    "        for dataset_part in [\"train\", \"test\"]:\n",
    "            part_file = os.path.join(raw_path, f'{dataset_part}.csv')\n",
    "            part_df = pd.read_csv(part_file, header=None)\n",
    "            part_df.columns = column_names\n",
    "\n",
    "            if dataset == 'yahoo_answers':\n",
    "                part_df = part_df[~part_df['answer'].isna()]\n",
    "                part_df['text'] = part_df.apply(lambda x:\n",
    "                                                f\"{x['question_title']} {x['question_content']} {x['answer']}\", axis=1)\n",
    "            elif dataset == 'ag_news':\n",
    "                part_df['text'] = part_df.apply(lambda x: f\"{x['title']}. {x['text']}\", axis=1)\n",
    "\n",
    "            part_df = part_df[~part_df['text'].isna()]\n",
    "            part_df['text'] = part_df['text'].apply(lambda x: clean_text(x))\n",
    "            part_df['label'] = part_df['label'].apply(lambda x: idx_to_class_name[x - 1])\n",
    "            if dataset_part == 'test':\n",
    "                part_df.to_csv(os.path.join(dataset_out_dir, f'test.csv'), index=False)\n",
    "\n",
    "                with open(os.path.join(dataset_out_dir, 'class_names.txt'), 'w') as f:\n",
    "                    f.writelines([class_name + '\\n' for class_name in sorted(part_df[\"label\"].unique())])\n",
    "            else:\n",
    "                part_df.to_csv(os.path.join(dataset_out_dir, 'unlabeled.csv'), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_isear():\n",
    "    dataset_name = 'isear'\n",
    "    dataset_out_dir = os.path.join(OUT_DIR, dataset_name)\n",
    "    os.makedirs(dataset_out_dir, exist_ok=True)\n",
    "\n",
    "    logging.info(f'processing {dataset_name} csv files')\n",
    "    df = pd.read_csv(os.path.join(RAW_DIR, 'isear', 'isear.csv'), sep='|', quotechar='\"', on_bad_lines='warn')\n",
    "    df = df[['SIT', 'Field1']]\n",
    "    df.columns = ['text', 'label']\n",
    "    df['text'] = df['text'].apply(lambda x: x.replace('á ', ''))\n",
    "    df[\"label\"] = df[\"label\"].apply(lambda x: get_label_name(dataset_name, x))\n",
    "\n",
    "    unlabeled_df, test_df = train_test_split(df, test_size=0.2)\n",
    "    unlabeled_df.to_csv(os.path.join(dataset_out_dir, 'unlabeled.csv'), index=False)\n",
    "    test_df.to_csv(os.path.join(dataset_out_dir, 'test.csv'), index=False)\n",
    "\n",
    "    with open(os.path.join(dataset_out_dir, 'class_names.txt'), 'w') as f:\n",
    "        f.writelines([class_name+'\\n' for class_name in sorted(test_df[\"label\"].unique())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb():\n",
    "    dataset_name = 'imdb'\n",
    "    dataset_out_dir = os.path.join(OUT_DIR, dataset_name)\n",
    "    os.makedirs(dataset_out_dir, exist_ok=True)\n",
    "\n",
    "    logging.info(f'processing {dataset_name} csv files')\n",
    "    raw_dir = os.path.join(RAW_DIR, 'imdb', 'aclImdb')\n",
    "    train = []\n",
    "    for label in ['pos', 'neg', 'unsup']:\n",
    "        for file in os.listdir(os.path.join(raw_dir, 'train', label)):\n",
    "            train.append({'text': open(os.path.join(raw_dir, 'train', label, file)).read().replace('<br />', ' '),\n",
    "                          'label': get_label_name(dataset_name, label) if label != 'unsup' else ''})\n",
    "    test = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        for file in os.listdir(os.path.join(raw_dir, 'test', label)):\n",
    "            test.append({'text': open(os.path.join(raw_dir, 'test', label, file)).read().replace('<br />', ' '),\n",
    "                         'label': get_label_name(dataset_name, label)})\n",
    "\n",
    "    unlabeled_df = pd.DataFrame(train)\n",
    "    test_df = pd.DataFrame(test)\n",
    "\n",
    "    unlabeled_df.to_csv(os.path.join(dataset_out_dir, 'unlabeled.csv'), index=False)\n",
    "    test_df.to_csv(os.path.join(dataset_out_dir, 'test.csv'), index=False)\n",
    "\n",
    "    with open(os.path.join(dataset_out_dir, 'class_names.txt'), 'w') as f:\n",
    "        f.writelines([class_name+'\\n' for class_name in sorted(test_df[\"label\"].unique())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s')\n",
    "\n",
    "dataset_to_download_url = \\\n",
    "    {\n",
    "        'isear': 'https://raw.githubusercontent.com/sinmaniphel/py_isear_dataset/master/isear.csv',\n",
    "        'ag_news': 'https://docs.google.com/uc?export=download&id=0Bz8a_Dbh9QhbUDNpeUdjb0wxRms',\n",
    "        'dbpedia': 'https://docs.google.com/uc?export=download&id=0Bz8a_Dbh9QhbQ2Vic1kxMmZZQ1k&confirm=t',\n",
    "        'yahoo_answers': 'https://docs.google.com/uc?export=download&id=0Bz8a_Dbh9Qhbd2JNdDBsQUdocVU&confirm=t',\n",
    "        'imdb': 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "    }\n",
    "\n",
    "for dataset, url in dataset_to_download_url.items():\n",
    "    out_dir = os.path.join(RAW_DIR, dataset)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    logging.info(f'downloading {dataset} raw files')\n",
    "    extension = '.'.join(url.split(os.sep)[-1].split('.')[1:])\n",
    "    if len(extension) == 0:\n",
    "        extension = 'tar.gz'\n",
    "    target_file = os.path.join(out_dir, f'{dataset}.{extension}')\n",
    "    urllib.request.urlretrieve(url, target_file)\n",
    "    if extension == 'tar.gz':\n",
    "        file = tarfile.open(target_file)\n",
    "        file.extractall(out_dir)\n",
    "        file.close()\n",
    "    elif extension == 'zip':\n",
    "        with zipfile.ZipFile(target_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(out_dir)\n",
    "\n",
    "load_20_newsgroup()\n",
    "load_ag_news_dbpedia_yahoo()\n",
    "load_isear()\n",
    "load_imdb()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--experiment_name', required=True)\n",
    "parser.add_argument('--dataset_name', required=True)\n",
    "parser.add_argument(\"--base_model\", required=True)\n",
    "\n",
    "parser.add_argument(\"--num_iterations\", type=int, default=2)\n",
    "parser.add_argument(\"--dataset_subset_size\", type=int, default=10000)\n",
    "parser.add_argument(\"--sample_ratio\", type=float, default=0.01)\n",
    "parser.add_argument(\"--negative_sampling_strategy\", default='take_random', type=str)\n",
    "\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=2e-5)\n",
    "parser.add_argument(\"--train_batch_size\", type=int, default=16)\n",
    "parser.add_argument(\"--infer_batch_size\", type=int, default=16)\n",
    "parser.add_argument(\"--max_length\", type=int, default=512)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--delete_models\", action='store_true')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = \"--experiment_name newsgroup_Lst --dataset_name 20_newsgroup \"\\\n",
    "\" --base_model roberta-large-mnli --seed 0 --dataset_subset_size 10000\"\\\n",
    "\" --train_batch_size 4 --infer_batch_size 4 --negative_sampling_strategy TAKE_LAST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(params.split())\n",
    "\n",
    "config_dict = vars(args)\n",
    "logging.info(config_dict)\n",
    "\n",
    "# This string describes the full self-training configuration, to ease aggregation across seeds\n",
    "setting_name = '_'.join([str(value) for key, value in config_dict.items()\n",
    "                            if key not in ['seed', 'infer_batch_size', 'delete_models']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "data_path = os.path.join(get_root_dir(), 'datasets', args.dataset_name)\n",
    "out_dir = os.path.join(get_root_dir(), 'output', 'experiments', args.experiment_name)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "unlabeled_df = pd.read_csv(os.path.join(data_path, 'unlabeled.csv'))\n",
    "unlabeled_texts = unlabeled_df['text']\n",
    "\n",
    "with open(os.path.join(data_path, 'class_names.txt')) as f:\n",
    "    class_names = f.read().splitlines()\n",
    "\n",
    "# Limit the size of the unlabeled set to reduce runtime\n",
    "subset_idxs = random.sample(range(len(unlabeled_texts)), min(args.dataset_subset_size, len(unlabeled_texts)))\n",
    "unlabeled_texts = [unlabeled_texts[idx] for idx in subset_idxs]\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
    "test_texts = test_df['text']             #.iloc[0:5000]\n",
    "test_gold_labels = test_df['label']      #.iloc[0:5000]\n",
    "\n",
    "# Set the desired number of pseudo-labeled positive examples per class\n",
    "sample_size = int(len(unlabeled_texts) * args.sample_ratio)\n",
    "logging.info(f'sample size per class is {sample_size}, set by a sample ratio of {args.sample_ratio}')\n",
    "\n",
    "model_name = args.base_model\n",
    "\n",
    "logging.info(f\"Evaluating base zero-shot model '{model_name}' performance on test set\")\n",
    "test_preds = get_zero_shot_predictions(model_name, test_texts, class_names,\n",
    "                                        batch_size=args.infer_batch_size, max_length=args.max_length)\n",
    "evaluate_classification_performance(test_preds.predicted_labels, test_gold_labels, out_dir,\n",
    "                                    info_dict={\n",
    "                                        'iteration': 0,\n",
    "                                        'setting_name': f'{setting_name}_base',\n",
    "                                        **config_dict\n",
    "                                    })\n",
    "\n",
    "for iter_number in range(1, args.num_iterations+1):\n",
    "    logging.info(f\"Inferring with zero-shot model '{model_name}' on {len(unlabeled_texts)} unlabeled elements)\")\n",
    "    predictions = get_zero_shot_predictions(model_name, unlabeled_texts, class_names,\n",
    "                                            batch_size=args.infer_batch_size, max_length=args.max_length)\n",
    "    logging.info(f\"Done inferring zero-shot model on {len(unlabeled_texts)} unlabeled elements\")\n",
    "\n",
    "    if args.delete_models and model_name != args.base_model:\n",
    "        logging.info(f\"deleting fine-tuned model {model_name}\")\n",
    "        shutil.rmtree(model_name)\n",
    "\n",
    "    self_training_set = SelfTrainingSet()\n",
    "    # For each class, we rank the elements as candidates for self-training according to the model confidence\n",
    "    class_name_to_sorted_idxs = rank_candidate_indices_per_class(class_names, predictions)\n",
    "\n",
    "    # We choose the <sample_size> best examples from each class as positive (entailment) examples\n",
    "    class_name_to_positive_chosen_idxs = {class_name: sorted_idxs[:sample_size]\n",
    "                                            for class_name, sorted_idxs in class_name_to_sorted_idxs.items()}\n",
    "\n",
    "    for class_name, idxs in class_name_to_positive_chosen_idxs.items():\n",
    "        self_training_set.texts.extend([unlabeled_texts[idx] for idx in idxs])\n",
    "        self_training_set.class_names.extend([class_name]*len(idxs))\n",
    "        self_training_set.entailment_labels.extend(['ENTAILMENT']*len(idxs))\n",
    "\n",
    "    # Add negative (contradiction) examples\n",
    "    negative_sampling_strategy = NegativeSamplingStrategy[args.negative_sampling_strategy.upper()]\n",
    "    class_name_to_negative_chosen_idxs = \\\n",
    "        get_negative_examples(predictions, class_name_to_positive_chosen_idxs, negative_sampling_strategy)\n",
    "\n",
    "    for class_name, idxs in class_name_to_negative_chosen_idxs.items():\n",
    "        self_training_set.texts.extend([unlabeled_texts[idx] for idx in idxs])\n",
    "        self_training_set.class_names.extend([class_name]*len(idxs))\n",
    "        self_training_set.entailment_labels.extend(['CONTRADICTION']*len(idxs))\n",
    "\n",
    "    logging.info(f\"Done collecting pseudo-labeled elements for self-training iteration {iter_number}: \"\n",
    "                    f\"{Counter(self_training_set.entailment_labels)}\")\n",
    "\n",
    "    # We use the updated pseudo-labeled set from this iteration to fine-tune the *base* entailment model\n",
    "    logging.info(f\"Fine-tuning model '{args.base_model}' on {len(self_training_set.entailment_labels)} \"\n",
    "                    f\"pseudo-labeled texts\")\n",
    "    finetuned_model_path = finetune_entailment_model(\n",
    "        model_name=args.base_model, self_training_set=self_training_set, seed=args.seed,\n",
    "        learning_rate=args.learning_rate, batch_size=args.train_batch_size, max_length=args.max_length,\n",
    "        num_epochs=1)\n",
    "    logging.info(f\"Done fine-tuning. Model for self-training iteration {iter_number} \"\n",
    "                    f\"saved to {finetuned_model_path}.\")\n",
    "\n",
    "    model_name = finetuned_model_path\n",
    "\n",
    "    logging.info(f'iteration {iter_number}: evaluating model {model_name} performance on test set')\n",
    "    test_preds = get_zero_shot_predictions(model_name, test_texts, class_names,\n",
    "                                            batch_size=args.infer_batch_size, max_length=args.max_length)\n",
    "    evaluate_classification_performance(test_preds.predicted_labels, test_gold_labels, out_dir,\n",
    "                                        info_dict={\n",
    "                                            'iteration': iter_number,\n",
    "                                            'setting_name': f'{setting_name}_iter_{iter_number}',\n",
    "                                            'self_training_set_size':  len(self_training_set.texts),\n",
    "                                            **config_dict\n",
    "                                        })"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
