{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from collections import defaultdict, Counter\n",
    "from enum import Enum\n",
    "from typing import Dict, List\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)-8s [%(filename)s:%(lineno)d] %(message)s')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from data_structs import SelfTrainingSet, Predictions\n",
    "from entailment_models import finetune_entailment_model, get_zero_shot_predictions\n",
    "from utils import set_seed, get_root_dir\n",
    "from evaluate import evaluate_classification_performance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank Candidate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidate_indices_per_class(all_class_names, predictions: Predictions) -> Dict[str, List[int]]:\n",
    "    diff_scores_to_second_best = \\\n",
    "        [class_name_to_score[ranked_classes[0]] - class_name_to_score[ranked_classes[1]]\n",
    "         for class_name_to_score, ranked_classes in zip(predictions.class_name_to_score, predictions.ranked_classes)]\n",
    "\n",
    "    class_name_to_sorted_candidate_idxs = {}\n",
    "    for class_name in all_class_names:\n",
    "        sorted_candidate_idxs = \\\n",
    "            [idx for idx, (predicted_class, diff_to_second_best)\n",
    "             in sorted(enumerate(zip(predictions.predicted_labels, diff_scores_to_second_best)),\n",
    "                       key=lambda x: x[1][1], reverse=True)\n",
    "             if predicted_class == class_name]\n",
    "        class_name_to_sorted_candidate_idxs[class_name] = sorted_candidate_idxs\n",
    "\n",
    "    return class_name_to_sorted_candidate_idxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Negative Sampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingStrategy(Enum):\n",
    "    TAKE_ALL = 0\n",
    "    TAKE_RANDOM = 1\n",
    "    TAKE_SECOND = 2\n",
    "    TAKE_LAST = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negative_examples(predictions: Predictions, class_name_to_chosen_pos_idxs: Dict[str, List[int]],\n",
    "                          negative_sampling_strategy: NegativeSamplingStrategy) -> Dict[str, List[int]]:\n",
    "\n",
    "    all_positive_idxs = [idx for class_idxs in class_name_to_chosen_pos_idxs.values() for idx in class_idxs]\n",
    "\n",
    "    class_name_to_chosen_negative_idxs = defaultdict(list)\n",
    "    for idx in all_positive_idxs:\n",
    "        example_ranked_classes = predictions.ranked_classes[idx]\n",
    "\n",
    "        if negative_sampling_strategy == NegativeSamplingStrategy.TAKE_SECOND:\n",
    "            class_name_to_chosen_negative_idxs[example_ranked_classes[1]].append(idx)\n",
    "        elif negative_sampling_strategy == NegativeSamplingStrategy.TAKE_ALL:\n",
    "            for class_name in example_ranked_classes[1:]:\n",
    "                class_name_to_chosen_negative_idxs[class_name].append(idx)\n",
    "        elif negative_sampling_strategy == NegativeSamplingStrategy.TAKE_LAST:\n",
    "            class_name_to_chosen_negative_idxs[example_ranked_classes[-1]].append(idx)\n",
    "        elif negative_sampling_strategy == NegativeSamplingStrategy.TAKE_RANDOM:\n",
    "            random_negative_class = random.choice(example_ranked_classes[1:])\n",
    "            class_name_to_chosen_negative_idxs[random_negative_class].append(idx)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown negative sampling strategy {negative_sampling_strategy}\")\n",
    "\n",
    "    return class_name_to_chosen_negative_idxs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser()\n",
    "parser.add_argument('--experiment_name', required=True)\n",
    "parser.add_argument('--dataset_name', required=True)\n",
    "parser.add_argument(\"--base_model\", required=True)\n",
    "\n",
    "parser.add_argument(\"--num_iterations\", type=int, default=2)\n",
    "parser.add_argument(\"--dataset_subset_size\", type=int, default=10000)\n",
    "parser.add_argument(\"--sample_ratio\", type=float, default=0.01)\n",
    "parser.add_argument(\"--negative_sampling_strategy\", default='take_random', type=str)\n",
    "\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=2e-5)\n",
    "parser.add_argument(\"--train_batch_size\", type=int, default=16)\n",
    "parser.add_argument(\"--infer_batch_size\", type=int, default=16)\n",
    "parser.add_argument(\"--max_length\", type=int, default=512)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "\n",
    "parser.add_argument(\"--delete_models\", action='store_true')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = \"--experiment_name newsgroup_Lst --dataset_name 20_newsgroup \"\\\n",
    "\" --base_model roberta-large-mnli --seed 0 --dataset_subset_size 10000\"\\\n",
    "\" --train_batch_size 4 --infer_batch_size 4 --negative_sampling_strategy TAKE_LAST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(params.split())\n",
    "\n",
    "config_dict = vars(args)\n",
    "logging.info(config_dict)\n",
    "\n",
    "# This string describes the full self-training configuration, to ease aggregation across seeds\n",
    "setting_name = '_'.join([str(value) for key, value in config_dict.items()\n",
    "                            if key not in ['seed', 'infer_batch_size', 'delete_models']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "data_path = os.path.join(get_root_dir(), 'datasets', args.dataset_name)\n",
    "out_dir = os.path.join(get_root_dir(), 'output', 'experiments', args.experiment_name)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "unlabeled_df = pd.read_csv(os.path.join(data_path, 'unlabeled.csv'))\n",
    "unlabeled_texts = unlabeled_df['text']\n",
    "\n",
    "with open(os.path.join(data_path, 'class_names.txt')) as f:\n",
    "    class_names = f.read().splitlines()\n",
    "\n",
    "# Limit the size of the unlabeled set to reduce runtime\n",
    "subset_idxs = random.sample(range(len(unlabeled_texts)), min(args.dataset_subset_size, len(unlabeled_texts)))\n",
    "unlabeled_texts = [unlabeled_texts[idx] for idx in subset_idxs]\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(data_path, 'test.csv'))\n",
    "test_texts = test_df['text']             #.iloc[0:5000]\n",
    "test_gold_labels = test_df['label']      #.iloc[0:5000]\n",
    "\n",
    "# Set the desired number of pseudo-labeled positive examples per class\n",
    "sample_size = int(len(unlabeled_texts) * args.sample_ratio)\n",
    "logging.info(f'sample size per class is {sample_size}, set by a sample ratio of {args.sample_ratio}')\n",
    "\n",
    "model_name = args.base_model\n",
    "\n",
    "logging.info(f\"Evaluating base zero-shot model '{model_name}' performance on test set\")\n",
    "test_preds = get_zero_shot_predictions(model_name, test_texts, class_names,\n",
    "                                        batch_size=args.infer_batch_size, max_length=args.max_length)\n",
    "evaluate_classification_performance(test_preds.predicted_labels, test_gold_labels, out_dir,\n",
    "                                    info_dict={\n",
    "                                        'iteration': 0,\n",
    "                                        'setting_name': f'{setting_name}_base',\n",
    "                                        **config_dict\n",
    "                                    })\n",
    "\n",
    "for iter_number in range(1, args.num_iterations+1):\n",
    "    logging.info(f\"Inferring with zero-shot model '{model_name}' on {len(unlabeled_texts)} unlabeled elements)\")\n",
    "    predictions = get_zero_shot_predictions(model_name, unlabeled_texts, class_names,\n",
    "                                            batch_size=args.infer_batch_size, max_length=args.max_length)\n",
    "    logging.info(f\"Done inferring zero-shot model on {len(unlabeled_texts)} unlabeled elements\")\n",
    "\n",
    "    if args.delete_models and model_name != args.base_model:\n",
    "        logging.info(f\"deleting fine-tuned model {model_name}\")\n",
    "        shutil.rmtree(model_name)\n",
    "\n",
    "    self_training_set = SelfTrainingSet()\n",
    "    # For each class, we rank the elements as candidates for self-training according to the model confidence\n",
    "    class_name_to_sorted_idxs = rank_candidate_indices_per_class(class_names, predictions)\n",
    "\n",
    "    # We choose the <sample_size> best examples from each class as positive (entailment) examples\n",
    "    class_name_to_positive_chosen_idxs = {class_name: sorted_idxs[:sample_size]\n",
    "                                            for class_name, sorted_idxs in class_name_to_sorted_idxs.items()}\n",
    "\n",
    "    for class_name, idxs in class_name_to_positive_chosen_idxs.items():\n",
    "        self_training_set.texts.extend([unlabeled_texts[idx] for idx in idxs])\n",
    "        self_training_set.class_names.extend([class_name]*len(idxs))\n",
    "        self_training_set.entailment_labels.extend(['ENTAILMENT']*len(idxs))\n",
    "\n",
    "    # Add negative (contradiction) examples\n",
    "    negative_sampling_strategy = NegativeSamplingStrategy[args.negative_sampling_strategy.upper()]\n",
    "    class_name_to_negative_chosen_idxs = \\\n",
    "        get_negative_examples(predictions, class_name_to_positive_chosen_idxs, negative_sampling_strategy)\n",
    "\n",
    "    for class_name, idxs in class_name_to_negative_chosen_idxs.items():\n",
    "        self_training_set.texts.extend([unlabeled_texts[idx] for idx in idxs])\n",
    "        self_training_set.class_names.extend([class_name]*len(idxs))\n",
    "        self_training_set.entailment_labels.extend(['CONTRADICTION']*len(idxs))\n",
    "\n",
    "    logging.info(f\"Done collecting pseudo-labeled elements for self-training iteration {iter_number}: \"\n",
    "                    f\"{Counter(self_training_set.entailment_labels)}\")\n",
    "\n",
    "    # We use the updated pseudo-labeled set from this iteration to fine-tune the *base* entailment model\n",
    "    logging.info(f\"Fine-tuning model '{args.base_model}' on {len(self_training_set.entailment_labels)} \"\n",
    "                    f\"pseudo-labeled texts\")\n",
    "    finetuned_model_path = finetune_entailment_model(\n",
    "        model_name=args.base_model, self_training_set=self_training_set, seed=args.seed,\n",
    "        learning_rate=args.learning_rate, batch_size=args.train_batch_size, max_length=args.max_length,\n",
    "        num_epochs=1)\n",
    "    logging.info(f\"Done fine-tuning. Model for self-training iteration {iter_number} \"\n",
    "                    f\"saved to {finetuned_model_path}.\")\n",
    "\n",
    "    model_name = finetuned_model_path\n",
    "\n",
    "    logging.info(f'iteration {iter_number}: evaluating model {model_name} performance on test set')\n",
    "    test_preds = get_zero_shot_predictions(model_name, test_texts, class_names,\n",
    "                                            batch_size=args.infer_batch_size, max_length=args.max_length)\n",
    "    evaluate_classification_performance(test_preds.predicted_labels, test_gold_labels, out_dir,\n",
    "                                        info_dict={\n",
    "                                            'iteration': iter_number,\n",
    "                                            'setting_name': f'{setting_name}_iter_{iter_number}',\n",
    "                                            'self_training_set_size':  len(self_training_set.texts),\n",
    "                                            **config_dict\n",
    "                                        })"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
